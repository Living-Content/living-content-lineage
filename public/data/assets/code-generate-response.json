{
  "claim_generator": "LivingContent/1.0.0",
  "claim_generator_info": [{ "name": "Living Content", "version": "1.0.0" }],
  "title": "Response Generator",
  "format": "application/vnd.livingcontent.code+python",
  "instance_id": "urn:uuid:code-generate-response",

  "assertions": [
    {
      "label": "c2pa.actions",
      "data": {
        "actions": [{
          "action": "c2pa.created",
          "digitalSourceType": "http://cv.iptc.org/newscodes/digitalsourcetype/algorithmicMedia",
          "softwareAgent": { "name": "Living Content Pipeline", "version": "1.0.0" },
          "when": "2026-01-14T18:15:30Z"
        }]
      }
    },
    {
      "label": "lco.code",
      "data": {
        "function": "_stream_llm_response",
        "module": "core.tasks.query",
        "computation": "generate_response",
        "hash": "sha256:ghi789abc123def456ghi789abc123def456ghi789abc123def456ghi789abc1"
      }
    },
    {
      "label": "lco.execution",
      "data": {
        "execution_start_time": "1768414502.7448637",
        "execution_end_time": "1768414514.774498",
        "execution_duration_ms": 12029.63,
        "previous_function": "retrieve_history",
        "next_function": "save_response"
      }
    }
  ],

  "source_code": "@chain_link(\n    computation=\"generate_response\",\n    input_asset_type=\"model\",\n    output_asset_type=\"document\",\n)\nasync def _stream_llm_response(\n    query_ctx,\n    system_prompt: str,\n    user_messages: list,\n):\n    \"\"\"Stream LLM response and accumulate full response text.\n\n    Yields:\n        Tuple[str, str, str, int]: (chunk, accumulated_full_response, model, max_tokens)\n    \"\"\"\n    benchmark = get_current_benchmark()\n\n    model_config = llm.get_model_config()\n    model = model_config[\"model\"]\n    max_tokens = model_config[\"max_tokens\"]\n    generation_client = model_config[\"generation_client\"]\n\n    set_model_metadata(\n        {\n            \"model\": model,\n            \"max_tokens\": max_tokens,\n        }\n    )\n\n    full_response = \"\"\n    async for chunk in llm.streaming_response(\n        query_ctx,\n        generation_client,\n        model,\n        max_tokens,\n        system_prompt,\n        user_messages,\n        query_ctx.content_session_id,\n    ):\n        if \"data: \" in chunk and not chunk.strip().endswith(\"[DONE]\"):\n            content = chunk.split(\"data: \", 1)[1].rstrip(\"\\n\")\n            if content:\n                full_response += content\n        yield chunk, full_response, model, max_tokens\n\n    if benchmark:\n        benchmark.log_info(f\"Full response length: {len(full_response)} chars\")",

  "claim": {
    "type": "certificate",
    "provider": "LCO",
    "alg": "ES256",
    "issuer": "did:key:zQ3shwc61yUNaJZBX2L9mZd3xhWjTEqD52dA3JxBnZnu78E3d",
    "time": "2026-01-14T18:15:30Z"
  }
}
